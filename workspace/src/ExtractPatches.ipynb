{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import openslide as op\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import util\n",
    "import h5py\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import misc, ndimage\n",
    "from skimage import morphology\n",
    "from skimage import color\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initArrays(files):\n",
    "    extracting = {}\n",
    "    none_extracting = {}\n",
    "    n_files = len(files)\n",
    "    for key, val in img_classes.items():\n",
    "        data[key] = {}\n",
    "        offset[key] = 0\n",
    "        if(val==None):\n",
    "            none_extracting[key]=0\n",
    "        elif(val<=0):\n",
    "            extracting[key]=0\n",
    "        else:\n",
    "            data[key + '_imgs'] = np.zeros((n_files*val,patchSize,patchSize,3)).astype(np.uint8)\n",
    "            data[key + '_masks'] = np.zeros((n_files*val,patchSize,patchSize, 1)).astype(np.uint8)\n",
    "    for file in files:\n",
    "        print(\"Working on \" + file)\n",
    "        name = os.path.splitext(os.path.basename(file))[0]\n",
    "        maskname = glob.glob(os.path.dirname(file)+\"/\"+name+maskPattern)[0]\n",
    "        im = op.OpenSlide(file)\n",
    "        imload = im.read_region((0,0), 1, im.level_dimensions[1])\n",
    "        mask = Image.open(maskname)\n",
    "        if(imload.size != mask.size):\n",
    "           mask = mask.resize(imload.size)\n",
    "        imArray = np.array(imload)\n",
    "        maskArray = np.array(mask)\n",
    "        maskArray= addBackground(imArray, maskArray)\n",
    "        del imArray\n",
    "        del im\n",
    "        del imload\n",
    "        for key, val in extracting.items():\n",
    "            nb_patches = (0 - img_classes[key]) + 1\n",
    "            maskClass = np.array(maskArray)\n",
    "            np.putmask(maskClass,maskClass!=classes[key],0)\n",
    "            maskClass = measure.label(maskClass)\n",
    "            nb_extract = 0\n",
    "            try:\n",
    "                if(max_classes[key]<maskClass.max()):\n",
    "                    print(\"Found \", maskClass.max(), \" components for \", key, \" but max number is \", max_classes[key])\n",
    "                else:\n",
    "                    print(\"Found \", maskClass.max(), \" components for \", key)\n",
    "                    max_classes[key] = maskClass.max()\n",
    "            except KeyError:\n",
    "                print(\"Found \", maskClass.max(), \" components for \", key)\n",
    "                max_classes[key] = maskClass.max()   \n",
    "            extracting[key] += max_classes[key] * nb_patches\n",
    "            print(\"New number of extraction for \" , key, \" : \", extracting[key])\n",
    "        max_extraction = 0\n",
    "        for key, val in extracting.items():\n",
    "            print(\"Size for \", key, \" is \", val)\n",
    "            data[key + '_imgs'] = np.zeros((val,patchSize,patchSize,3)).astype(np.uint8)\n",
    "            data[key + '_masks'] = np.zeros((val,patchSize,patchSize, 1)).astype(np.uint8)\n",
    "            if(val>max_extraction):\n",
    "                max_extraction=val\n",
    "        for key, val in none_extracting.items():\n",
    "            img_classes[key]=int(max_extraction/n_files)\n",
    "            print(\"Number of extraction per file for \", key, \" is \", img_classes[key])\n",
    "            data[key + '_imgs'] = np.zeros((max_extraction,patchSize,patchSize,3)).astype(np.uint8)\n",
    "            data[key + '_masks'] = np.zeros((max_extraction,patchSize,patchSize, 1)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addBackground(imArray, maskArray):\n",
    "    \"\"\"\n",
    "        Find the background on the array and put the value 2 on the mask\n",
    "    \"\"\"\n",
    "    im_in = cv2.cvtColor(imArray,cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold.\n",
    "    # Set values equal to or above 220 to 0.\n",
    "    # Set values below 220 to 255.\n",
    "    th, im_th = cv2.threshold(im_in, 220, 255, cv2.THRESH_BINARY_INV);\n",
    "    # Copy the thresholded image.\n",
    "    im_floodfill = im_th.copy()\n",
    "    # Mask used to flood filling.\n",
    "    # Notice the size needs to be 2 pixels than the image.\n",
    "    h, w = im_th.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 255);\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    "    # Combine the two images to get the foreground.\n",
    "    im_out = im_th | im_floodfill_inv\n",
    "    # Remove the small parts\n",
    "    kernel = np.ones((int(maskArray.shape[0]/115),int(maskArray.shape[1]/115)),np.uint8)\n",
    "    opening = cv2.morphologyEx(im_out, cv2.MORPH_OPEN, kernel)\n",
    "    # Invert the mask\n",
    "    np.putmask(opening,opening==0,2)\n",
    "    np.putmask(opening,opening==255,0)\n",
    "    opening += maskArray\n",
    "    return opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractPatches(output,filename,maskname, save=True):\n",
    "    \"\"\"\n",
    "        Extract the patches for the given file and maskname\n",
    "    \"\"\"\n",
    "    global j\n",
    "    global data\n",
    "    global offset\n",
    "    # Opening the files\n",
    "    im = op.OpenSlide(filename)\n",
    "    imload = im.read_region((0,0), 1, im.level_dimensions[1])\n",
    "    mask = Image.open(maskname)\n",
    "    if(imload.size != mask.size):\n",
    "       mask = mask.resize(imload.size)\n",
    "    imArray = np.array(imload)\n",
    "    maskArray = np.array(mask)\n",
    "    halfPatch = patchSize//2\n",
    " \n",
    "    #Preprocess\n",
    "    maskArray_back = addBackground(imArray, maskArray)\n",
    "    imArray = np.lib.pad(imArray, ((halfPatch, halfPatch), (halfPatch, halfPatch),(0,0)), 'reflect')\n",
    "    maskArrayPad = np.lib.pad(maskArray, ((halfPatch, halfPatch), (halfPatch, halfPatch)), 'reflect')\n",
    "    np.putmask(maskArrayPad, maskArrayPad==1, 255)\n",
    "    # Extraction\n",
    "    for key, val in classes.items():\n",
    "        print(\"Extracting patches for \", key)\n",
    "        if(img_classes[key]>0):\n",
    "            indices = np.where(maskArray_back==val)\n",
    "            sample = random.sample(range(len(indices[0])), img_classes[key])\n",
    "            maskClass = np.array(maskArrayPad) #TODO : remove this ?  \n",
    "            for i in sample:\n",
    "                x=indices[0][i]\n",
    "                y=indices[1][i]\n",
    "                x2 = x+patchSize\n",
    "                y2 = y+patchSize\n",
    "                croppedIm = imArray[x:x2,y:y2,0:3]\n",
    "                croppedMask = maskClass[x:x2,y:y2]              \n",
    "                # create the images if needed\n",
    "                if(save):\n",
    "                    imageName = output +  \"/image_\" + str(j) + \"_\" + key + \".png\"\n",
    "                    imageNameMask = output  + \"/image_\" + str(j) + \"_\" + key +\"_mask.png\"\n",
    "                    misc.imsave(imageName,croppedIm)\n",
    "                    misc.imsave(imageNameMask,croppedMask)\n",
    "                    os.chmod(imageName , 0o777)\n",
    "                    os.chmod(imageNameMask, 0o777)\n",
    "                # concatenate to the arrays\n",
    "                data[key + '_imgs'][offset[key],:,:,:]=croppedIm\n",
    "                data[key + '_masks'][offset[key],:,:,:]=np.expand_dims(croppedMask,-1)\n",
    "                offset[key] += 1\n",
    "                j+=1\n",
    "                if(j%100==0):\n",
    "                    print(\"\",j,\" patches extracted\")\n",
    "        else:\n",
    "            nb_patches = (0 - img_classes[key]) + 1\n",
    "            maskClass = np.array(maskArray_back)\n",
    "            np.putmask(maskClass,maskClass!=val,0)\n",
    "            maskClass = measure.label(maskClass)\n",
    "            print(\"Found \", maskClass.max(), \" components for \", key)\n",
    "            print(\"Extracting \",  max_classes[key], \"\")\n",
    "            patches = np.zeros((maskClass.max() * nb_patches,patchSize,patchSize,3))\n",
    "            mask_patches = np.zeros((maskClass.max() * nb_patches,patchSize,patchSize,1))\n",
    "            in_patches = 0\n",
    "            for i in (range(1,max_classes[key]+1)):\n",
    "                indices = np.where(maskClass==i)\n",
    "                x_center = int((indices[0].min() + indices[0].max()) / 2) + patchSize/2\n",
    "                y_center = int((indices[1].min() + indices[1].max()) / 2) + patchSize/2\n",
    "                division = 2\n",
    "                for h in range(0,nb_patches):\n",
    "                    x = x_center\n",
    "                    y = y_center\n",
    "                    if(h==0):\n",
    "                        None\n",
    "                    elif(h==1):\n",
    "                        y -= patchSize/division\n",
    "                    elif(h==2):\n",
    "                        x -= patchSize/division\n",
    "                    elif(h==3):\n",
    "                        y += patchSize/division\n",
    "                    elif(h==4):\n",
    "                        x += patchSize/division\n",
    "                    elif(h==5):\n",
    "                        x -= patchSize/division\n",
    "                        y -= patchSize/division\n",
    "                    elif(h==6):\n",
    "                        x -= patchSize/division\n",
    "                        y += patchSize/division\n",
    "                    elif(h==7):\n",
    "                        x += patchSize/division\n",
    "                        y += patchSize/division\n",
    "                    elif(h==8):\n",
    "                        x += patchSize/division\n",
    "                        y -= patchSize/division\n",
    "                    else:\n",
    "                        x += random.randint(-patchSize/division,patchSize/division)     \n",
    "                        y += random.randint(-patchSize/division,patchSize/division)\n",
    "                    x1 = int(x-patchSize/2)\n",
    "                    x2 = int(x+patchSize/2)\n",
    "                    y1 = int(y-patchSize/2)\n",
    "                    y2 = int(y+patchSize/2)\n",
    "                    croppedIm = imArray[x1:x2,y1:y2,0:3]\n",
    "                    croppedMask = maskArrayPad[x1:x2,y1:y2]\n",
    "                    if(save):\n",
    "                        imageName = output +  \"/image_\" + str(j) + \"_\" + key + \".png\"\n",
    "                        imageNameMask = output  + \"/image_\" + str(j) + \"_\" + key +\"_mask.png\"\n",
    "                        misc.imsave(imageName,croppedIm)\n",
    "                        misc.imsave(imageNameMask,croppedMask)\n",
    "                        os.chmod(imageName , 0o777)\n",
    "                        os.chmod(imageNameMask, 0o777)\n",
    "                    patches[in_patches,:,:,:]=croppedIm\n",
    "                    mask_patches[in_patches,:,:,:]=np.expand_dims(croppedMask,-1)\n",
    "                    in_patches+=1\n",
    "                    j+=1\n",
    "                    if(j%100==0):\n",
    "                        print(\"\",j,\" patches extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFiles(files, outputFolder, save=False):\n",
    "    \"\"\"\n",
    "        Extract all the files of a folder\n",
    "    \"\"\"\n",
    "    global data\n",
    "    global offset\n",
    "    j = 0\n",
    "    #initialize arrays\n",
    "    initArrays(files)\n",
    "    for oneFile in files:\n",
    "        name = os.path.splitext(os.path.basename(oneFile))[0]\n",
    "        for key, val in classes.items():\n",
    "            folder = outputFolder\n",
    "            if not os.path.exists(folder):\n",
    "                try:\n",
    "                  original_umask = os.umask(0)\n",
    "                  os.makedirs(folder,0o777)\n",
    "                finally:\n",
    "                  os.umask(original_umask)\n",
    "        print(\"Extracting \" + name)\n",
    "        maskFile = glob.glob(os.path.dirname(oneFile)+\"/\"+name+maskPattern)[0]\n",
    "        extractPatches(outputFolder, oneFile,maskFile, save)\n",
    "        print(\"Extraction for \", name, \" finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeAndSave(outputFile):\n",
    "    \"\"\"\n",
    "        Normalize the data and save it in a file\n",
    "    \"\"\"\n",
    "    # TODO : GROS FDP PASSE TOUT EN FLOAT 32\n",
    "    global data\n",
    "    all_data = np.zeros((j,patchSize,patchSize,3))\n",
    "    i = 0\n",
    "    # concatenate all the classes in one array to get the mean and std\n",
    "    for key, val in img_classes.items():\n",
    "        data[key+\"_imgs\"] = data[key+\"_imgs\"].astype(np.float32)/255\n",
    "        data[key+\"_masks\"] = data[key+\"_masks\"].astype(np.float32)\n",
    "        n = data[key+\"_imgs\"].shape[0] + i\n",
    "        print(\"Writing from \", i, \" to \", n, \" for \", key)\n",
    "        all_data[i:n,:,:,:]= data[key+\"_imgs\"]\n",
    "        i = n\n",
    "    mean = all_data.mean()\n",
    "    std = all_data.std()\n",
    "    stats = np.zeros(2)\n",
    "    stats[0] = mean\n",
    "    stats[1] = std\n",
    "    # Normalize the data\n",
    "    for key, val in img_classes.items():\n",
    "        data[key+\"_imgs\"] -= mean\n",
    "        data[key+\"_imgs\"] /= std\n",
    "    print(\"Mean : \", stats[0])\n",
    "    print(\"Std : \", stats[1])\n",
    "    # Create the file\n",
    "    f = h5py.File(outputFile,\"w\")\n",
    "    f.create_dataset(\"stats\", data=stats)\n",
    "    for key, val in classes.items():\n",
    "        f.create_dataset(key+\"_imgs\", data=data[key+\"_imgs\"])\n",
    "        f.create_dataset(key+\"_masks\", data=data[key+\"_masks\"])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patchSize = 256 # Size of the patches\n",
    "filenames_train = ['/root/workspace/data/SVS_train/*.svs'] # Files we will extract for training. You can use pattern suchs as *.svs\n",
    "outputFolder_train = '/root/workspace/data/new_extract/' # Output folder for training\n",
    "maskPattern= '*.png' # Pattern for the maskfilesoutputFile,\n",
    "classes = {'neg': 0, 'back' : 2, 'pos' : 1} # Classes and their value\n",
    "img_classes = {'neg' : None ,'back':5, 'pos' : -4} # Number of patches to extract for each class\n",
    "max_classes = {'pos': 100}\n",
    "save = True # if true the images will be save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j=0 # Counting variable\n",
    "data = {} # Dict with all data\n",
    "offset = {} # Dict with the offset per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/workspace/data/SVS_train/IFTA_14_02.svs', '/root/workspace/data/SVS_train/IFTA_15_02.svs', '/root/workspace/data/SVS_train/IFTA_11_02.svs', '/root/workspace/data/SVS_train/IFTA_13_02.svs', '/root/workspace/data/SVS_train/IFTA_10_02.svs']\n"
     ]
    }
   ],
   "source": [
    "# Get the files for training\n",
    "files_train = []\n",
    "for filename in filenames_train:\n",
    "    files_train.extend(glob.glob(filename))\n",
    "print(files_train)\n",
    "files_train=files_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on /root/workspace/data/SVS_train/IFTA_14_02.svs\n",
      "Found  168  components for  pos  but max number is  100\n",
      "New number of extraction for  pos  :  500\n",
      "Size for  pos  is  500\n",
      "Number of extraction per file for  neg  is  500\n",
      "Extracting IFTA_14_02\n",
      "Found  168  components for  pos\n",
      "Extracting  100 \n",
      " 100  patches extracted\n",
      " 200  patches extracted\n",
      " 300  patches extracted\n",
      " 400  patches extracted\n",
      " 500  patches extracted\n",
      " 600  patches extracted\n",
      " 700  patches extracted\n",
      " 800  patches extracted\n",
      " 900  patches extracted\n",
      " 1000  patches extracted\n",
      "Extraction for  IFTA_14_02  finished\n"
     ]
    }
   ],
   "source": [
    "# Training extraction\n",
    "extractFiles(files_train,outputFolder_train, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing from  0  to  5  for  back\n",
      "Writing from  5  to  505  for  pos\n",
      "Writing from  505  to  1005  for  neg\n",
      "Mean :  0.348024833708\n",
      "Std :  0.368327307502\n"
     ]
    }
   ],
   "source": [
    "# Normalize and save the data\n",
    "normalizeAndSave(outputFolder_train+\"matrice_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
